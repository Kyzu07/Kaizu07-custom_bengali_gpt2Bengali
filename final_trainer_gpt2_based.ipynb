{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7257462,"sourceType":"datasetVersion","datasetId":4205659},{"sourceId":7258214,"sourceType":"datasetVersion","datasetId":4206151}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:40:39.308544Z","iopub.execute_input":"2023-12-22T04:40:39.309233Z","iopub.status.idle":"2023-12-22T04:40:39.313361Z","shell.execute_reply.started":"2023-12-22T04:40:39.309193Z","shell.execute_reply":"2023-12-22T04:40:39.312378Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"!pip install pandas==1.5.3","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:40:39.659920Z","iopub.execute_input":"2023-12-22T04:40:39.660679Z","iopub.status.idle":"2023-12-22T04:40:52.367558Z","shell.execute_reply.started":"2023-12-22T04:40:39.660641Z","shell.execute_reply":"2023-12-22T04:40:52.366382Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pandas==1.5.3 in /opt/conda/lib/python3.10/site-packages (1.5.3)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas==1.5.3) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas==1.5.3) (2023.3)\nRequirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from pandas==1.5.3) (1.24.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas==1.5.3) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"def save_combined_text(dir_path, output_file):\n    combined_text = \"\"\n    for file_name in os.listdir(dir_path):\n        if file_name.endswith(\".txt\"):\n            with open(os.path.join(dir_path, file_name), 'r', encoding='utf-8') as file:\n                file_content = file.read()\n                combined_text += file_content + \"<|endoftext|>\" + \"\\n\"\n    with open(output_file, 'w', encoding='utf-8') as output:\n        output.write(combined_text)\n    print(\"Combined text saved to:\", output_file)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-22T04:40:52.369926Z","iopub.execute_input":"2023-12-22T04:40:52.370221Z","iopub.status.idle":"2023-12-22T04:40:52.376988Z","shell.execute_reply.started":"2023-12-22T04:40:52.370195Z","shell.execute_reply":"2023-12-22T04:40:52.376035Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"dir_path_train = \"/kaggle/input/bl-splitted2/train\"\noutput_file_train = \"/kaggle/working/bengali_literature_train_gpt2.txt\"\nsave_combined_text(dir_path_train, output_file_train)\n\ndir_path_valid = \"/kaggle/input/bl-splitted2/valid\"\noutput_file_valid = \"/kaggle/working/bengali_literature_valid_gpt2.txt\"\nsave_combined_text(dir_path_valid, output_file_valid)","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:40:52.378269Z","iopub.execute_input":"2023-12-22T04:40:52.378882Z","iopub.status.idle":"2023-12-22T04:40:54.197644Z","shell.execute_reply.started":"2023-12-22T04:40:52.378852Z","shell.execute_reply":"2023-12-22T04:40:54.196489Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Combined text saved to: /kaggle/working/bengali_literature_train_gpt2.txt\nCombined text saved to: /kaggle/working/bengali_literature_valid_gpt2.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\ngpt2_bengali_tokenizer = AutoTokenizer.from_pretrained(\"flax-community/gpt2-bengali\")","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:40:54.199857Z","iopub.execute_input":"2023-12-22T04:40:54.200182Z","iopub.status.idle":"2023-12-22T04:40:54.806778Z","shell.execute_reply.started":"2023-12-22T04:40:54.200154Z","shell.execute_reply":"2023-12-22T04:40:54.805912Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"print(len(gpt2_bengali_tokenizer.get_vocab()))\ngpt2_bengali_tokenizer","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:40:54.807951Z","iopub.execute_input":"2023-12-22T04:40:54.808277Z","iopub.status.idle":"2023-12-22T04:40:54.847574Z","shell.execute_reply.started":"2023-12-22T04:40:54.808250Z","shell.execute_reply":"2023-12-22T04:40:54.846519Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"50256\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"GPT2TokenizerFast(name_or_path='flax-community/gpt2-bengali', vocab_size=50256, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n\t0: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}"},"metadata":{}}]},{"cell_type":"code","source":"# new tokenizer\n# from tokenizers import (decoders, models, normalizers, pre_tokenizers, processors, trainers, Tokenizer)\n# from transformers import GPT2Tokenizer, GPT2TokenizerFast, GPT2Model, GPT2LMHeadModel\n# from transformers import TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n\n# new_tokenizer = Tokenizer(models.BPE())\n# new_tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n# trainer = trainers.BpeTrainer(vocab_size=5000, special_tokens=[\"<|endoftext|>\"])\n# train_file = '/kaggle/working/bengali_literature_train_gpt2.txt'\n# new_tokenizer.train([train_file], trainer=trainer)\n# new_tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n# new_tokenizer.decoder = decoders.ByteLevel()\n\n# new_tokenizer = GPT2TokenizerFast(tokenizer_object=new_tokenizer)\n# new_tokenizer.save_pretrained(\"new_tokenizer_gpt2\")\n# new_tokenizer\n\n# # gpt2 tokenizer\n# gpt2_tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n# print(len(gpt2_tokenizer.get_vocab()))\n# gpt2_tokenizer\n\n# # merge the vocabulary for the extended tokenizer\n# vocab_tokens = list(new_tokenizer.get_vocab())\n# decoded_tokens = [new_tokenizer.decoder.decode([token]) for token in vocab_tokens]\n# print(len(vocab_tokens), len(decoded_tokens))\n# gpt2_tokenizer.add_tokens(decoded_tokens)\n# gpt2_tokenizer.save_pretrained(\"extended_tokenizer_gpt2\")\n# print(len(gpt2_tokenizer.get_vocab()))\n# gpt2_tokenizer\n\n# # validate the changes\n# text = \"‡¶ï‡¶´‡¶ø, ‡¶ó‡¶æ‡¶® ‡¶Ü‡¶∞ ‡¶ï‡¶æ‡¶ú ‡¶®‡¶ø‡ßü‡ßá \"\n# gpt2_tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n# new_tokenizer = GPT2TokenizerFast.from_pretrained(\"new_tokenizer_gpt2\")\n# extended_tokenizer = GPT2TokenizerFast.from_pretrained(\"extended_tokenizer_gpt2\")\n\n# print(len(gpt2_tokenizer.encode(text)))\n# print(gpt2_tokenizer.encode(text))\n# print(len(new_tokenizer.encode(text)))\n# print(new_tokenizer.encode(text))\n# print(len(extended_tokenizer.encode(text)))\n# print(extended_tokenizer.encode(text))","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:40:54.849091Z","iopub.execute_input":"2023-12-22T04:40:54.849440Z","iopub.status.idle":"2023-12-22T04:40:54.859797Z","shell.execute_reply.started":"2023-12-22T04:40:54.849413Z","shell.execute_reply":"2023-12-22T04:40:54.858881Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"import logging\nimport time\nfrom datasets import Dataset, DatasetDict\n\n# Suppress the warning messages\nlogging.getLogger(\"transformers.tokenization_utils_base\").setLevel(logging.ERROR)\n\n# load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"flax-community/gpt2-bengali\")\ntokenizer.pad_token = tokenizer.eos_token\nprint(tokenizer.vocab_size)\nprint(len(tokenizer))","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:43:15.846992Z","iopub.execute_input":"2023-12-22T04:43:15.847416Z","iopub.status.idle":"2023-12-22T04:43:16.373741Z","shell.execute_reply.started":"2023-12-22T04:43:15.847382Z","shell.execute_reply":"2023-12-22T04:43:16.372771Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"50256\n50256\n","output_type":"stream"}]},{"cell_type":"code","source":"# Fine-tuning data\nfrom datasets import Dataset, DatasetDict\n\ntrain_file = \"/kaggle/working/bengali_literature_train_gpt2.txt\"  # Replace with your training data file\nvalid_file = \"/kaggle/working/bengali_literature_valid_gpt2.txt\"  # Replace with your validation data file\n# Read the text data from the .txt file\nwith open(train_file, 'r', encoding='utf-8') as f:\n    train_data = f.readlines()\nwith open(valid_file, 'r', encoding='utf-8') as f:\n    valid_data = f.readlines()\n\nprint(len(train_data), len(valid_data))","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:43:17.952989Z","iopub.execute_input":"2023-12-22T04:43:17.953407Z","iopub.status.idle":"2023-12-22T04:43:18.712883Z","shell.execute_reply.started":"2023-12-22T04:43:17.953373Z","shell.execute_reply":"2023-12-22T04:43:18.711831Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"901371 9296\n","output_type":"stream"}]},{"cell_type":"code","source":"# Create a Dataset object from the text data\ntrain_dataset = Dataset.from_dict({\"text\": train_data})\nvalid_dataset = Dataset.from_dict({\"text\": valid_data})\n\ndef preprocess_function(examples):\n    out = tokenizer(examples[\"text\"])\n    # out = tokenizer([\" \".join(x) for x in examples[\"text\"]])\n    return out","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:43:34.214927Z","iopub.execute_input":"2023-12-22T04:43:34.215312Z","iopub.status.idle":"2023-12-22T04:43:35.615359Z","shell.execute_reply.started":"2023-12-22T04:43:34.215280Z","shell.execute_reply":"2023-12-22T04:43:35.614486Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"train_dataset","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:43:35.616964Z","iopub.execute_input":"2023-12-22T04:43:35.617285Z","iopub.status.idle":"2023-12-22T04:43:35.623772Z","shell.execute_reply.started":"2023-12-22T04:43:35.617257Z","shell.execute_reply":"2023-12-22T04:43:35.622789Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['text'],\n    num_rows: 901371\n})"},"metadata":{}}]},{"cell_type":"code","source":"# Apply tokenization to the dataset in batches using map\ntrain_dataset = train_dataset.map(preprocess_function, batched=True, num_proc=4, remove_columns=train_dataset.column_names)\ntime.sleep(5)\nvalid_dataset = valid_dataset.map(preprocess_function, batched=True, num_proc=4, remove_columns=valid_dataset.column_names)\ntokenized_datasets = DatasetDict({\"train\": train_dataset, \"valid\":valid_dataset})\n","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:43:39.424684Z","iopub.execute_input":"2023-12-22T04:43:39.425575Z","iopub.status.idle":"2023-12-22T04:45:50.839078Z","shell.execute_reply.started":"2023-12-22T04:43:39.425539Z","shell.execute_reply":"2023-12-22T04:45:50.837915Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"     ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#0:   0%|          | 0/226 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b985efdd67e4467ba3987f9ff47e34bb"}},"metadata":{}},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#1:   0%|          | 0/226 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c229a556bf34d228e3407346f7e242c"}},"metadata":{}},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#2:   0%|          | 0/226 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e311e2039d6465087f6418ee06e371e"}},"metadata":{}},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#3:   0%|          | 0/226 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2d5697a0609420ba159046362dd2378"}},"metadata":{}},{"name":"stdout","text":"       ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#0:   0%|          | 0/3 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b30f66af6fd54ed1aee5cde04b5b7342"}},"metadata":{}},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#2:   0%|          | 0/3 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7798e81e396e4bfc80194ddbc4d796e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#1:   0%|          | 0/3 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bd2718b13544801bc8f9d51f312d963"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#3:   0%|          | 0/3 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8921bb28585e4e58939bfed3a6041952"}},"metadata":{}}]},{"cell_type":"code","source":"train_dataset","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:45:50.841531Z","iopub.execute_input":"2023-12-22T04:45:50.842375Z","iopub.status.idle":"2023-12-22T04:45:50.848942Z","shell.execute_reply.started":"2023-12-22T04:45:50.842333Z","shell.execute_reply":"2023-12-22T04:45:50.847890Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'attention_mask'],\n    num_rows: 901371\n})"},"metadata":{}}]},{"cell_type":"code","source":"# Group the tokenized datasets into blocks \nblock_size = 128\n\ndef group_texts(examples):\n    # Concatenate all texts.\n    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n    total_length = len(concatenated_examples[list(examples.keys())[0]])\n    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n    # customize this part to your needs.\n    if total_length >= block_size:\n        total_length = (total_length // block_size) * block_size\n    # Split by chunks of block_size.\n    result = {\n        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n        for k, t in concatenated_examples.items()\n    }\n    result[\"labels\"] = result[\"input_ids\"].copy() # First note that we duplicate the inputs for our labels. This is because the model of the ü§ó Transformers library apply the shifting to the right, so we don't need to do it manually.\n    return result","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:45:50.850195Z","iopub.execute_input":"2023-12-22T04:45:50.850459Z","iopub.status.idle":"2023-12-22T04:45:50.860827Z","shell.execute_reply.started":"2023-12-22T04:45:50.850435Z","shell.execute_reply":"2023-12-22T04:45:50.860105Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Group the tokenized train dataset\nlm_train_dataset = tokenized_datasets['train'].map(group_texts, batched=True, num_proc=4)\ntime.sleep(2)\n# Group the tokenized valid dataset\nlm_valid_dataset = tokenized_datasets['valid'].map(group_texts, batched=True, num_proc=4)\n\nlm_dataset = DatasetDict({\"train\": lm_train_dataset, \"valid\":lm_valid_dataset})\nlm_dataset","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:45:50.863064Z","iopub.execute_input":"2023-12-22T04:45:50.863361Z","iopub.status.idle":"2023-12-22T04:49:14.696923Z","shell.execute_reply.started":"2023-12-22T04:45:50.863336Z","shell.execute_reply":"2023-12-22T04:49:14.695777Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"     ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#0:   0%|          | 0/226 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26e0087bafea49c3a331ea0a629c6b6c"}},"metadata":{}},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#1:   0%|          | 0/226 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5441071f743c4d5380ae9e19f0fb3a81"}},"metadata":{}},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#2:   0%|          | 0/226 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24542f2e3aee4715a34ced17058ce290"}},"metadata":{}},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#3:   0%|          | 0/226 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"296d22edaf3e4bf09f52457733abeb9a"}},"metadata":{}},{"name":"stdout","text":"      ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#0:   0%|          | 0/3 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"986b9b7b4a734ebcac2f533fcb595f54"}},"metadata":{}},{"name":"stdout","text":"  ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#1:   0%|          | 0/3 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"681672f9507a4173bcec369a815ae9d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#2:   0%|          | 0/3 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"876bad5837e440d6813bdba96268eacb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#3:   0%|          | 0/3 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89d1ad72d14a4e84803dfd917a759cca"}},"metadata":{}},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 373904\n    })\n    valid: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 3488\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"lm_dataset","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:33:33.479666Z","iopub.status.idle":"2023-12-22T04:33:33.480089Z","shell.execute_reply.started":"2023-12-22T04:33:33.479897Z","shell.execute_reply":"2023-12-22T04:33:33.479917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import DataCollatorForLanguageModeling","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:49:52.130483Z","iopub.execute_input":"2023-12-22T04:49:52.130956Z","iopub.status.idle":"2023-12-22T04:50:08.153790Z","shell.execute_reply.started":"2023-12-22T04:49:52.130918Z","shell.execute_reply":"2023-12-22T04:50:08.152895Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"# Data collator for language modeling\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\nout = data_collator([lm_dataset['train'][i] for i in range(5)])\nfor key in out:\n    print(f\"{key} shape: {out[key].shape}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:50:14.081927Z","iopub.execute_input":"2023-12-22T04:50:14.083165Z","iopub.status.idle":"2023-12-22T04:50:14.221865Z","shell.execute_reply.started":"2023-12-22T04:50:14.083121Z","shell.execute_reply":"2023-12-22T04:50:14.220766Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"input_ids shape: torch.Size([5, 128])\nattention_mask shape: torch.Size([5, 128])\nlabels shape: torch.Size([5, 128])\n","output_type":"stream"}]},{"cell_type":"code","source":"lm_dataset","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:50:20.732229Z","iopub.execute_input":"2023-12-22T04:50:20.732980Z","iopub.status.idle":"2023-12-22T04:50:20.739532Z","shell.execute_reply.started":"2023-12-22T04:50:20.732941Z","shell.execute_reply":"2023-12-22T04:50:20.738310Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 373904\n    })\n    valid: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 3488\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"# Option 1: Load the GPT-2 base model and resize its embeddings to match the vocabulary size of our custom tokenizer. \nfrom transformers import GPT2LMHeadModel\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load the GPT-2 base model\nmodel = GPT2LMHeadModel.from_pretrained(\"flax-community/gpt2-bengali\").to(device)\n\n# Resize the model's embeddings to match the vocabulary size of our tokenizer\n# def find_multiple(n: int, k: int) -> int:\n#     if n % k == 0:\n#         return n\n#     return n + k - (n % k)\n\n# new_embeddings_size = find_multiple(len(tokenizer), 64)\n# model.resize_token_embeddings(new_embeddings_size)","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:50:23.589749Z","iopub.execute_input":"2023-12-22T04:50:23.590665Z","iopub.status.idle":"2023-12-22T04:50:35.936785Z","shell.execute_reply.started":"2023-12-22T04:50:23.590628Z","shell.execute_reply":"2023-12-22T04:50:35.935635Z"},"trusted":true},"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/510M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8547953a9044e348799875e9729d148"}},"metadata":{}}]},{"cell_type":"code","source":"# # Option 2: freeze certain layers of the model (e.g., except embeddings)\nfreeze_layers = False\n\nif freeze_layers:\n    for name, param in model.named_parameters():\n        if 'transformer.wte' in name:\n            param.requires_grad = True\n        else:\n            param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:50:35.938887Z","iopub.execute_input":"2023-12-22T04:50:35.939196Z","iopub.status.idle":"2023-12-22T04:50:35.944513Z","shell.execute_reply.started":"2023-12-22T04:50:35.939171Z","shell.execute_reply":"2023-12-22T04:50:35.943652Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"custom_bengali_gpt2Bengali/\",\n    overwrite_output_dir=True,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    evaluation_strategy=\"steps\",\n    eval_steps=300,\n    logging_steps=300,\n    gradient_accumulation_steps=4,\n    num_train_epochs=3,\n    weight_decay=0.1,\n    warmup_steps=300,\n    lr_scheduler_type=\"cosine\",\n    learning_rate=5e-4,\n    save_steps=300,\n    push_to_hub=False,\n    save_total_limit=2\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:50:35.945690Z","iopub.execute_input":"2023-12-22T04:50:35.947501Z","iopub.status.idle":"2023-12-22T04:50:36.035918Z","shell.execute_reply.started":"2023-12-22T04:50:35.947446Z","shell.execute_reply":"2023-12-22T04:50:36.034703Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# Initialize the Trainer\ntrainer = Trainer(\n    model=model,\n    args=args,\n    data_collator=data_collator,\n    train_dataset=lm_dataset[\"train\"],\n    eval_dataset=lm_dataset[\"valid\"],\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:50:36.038398Z","iopub.execute_input":"2023-12-22T04:50:36.038866Z","iopub.status.idle":"2023-12-22T04:50:37.056287Z","shell.execute_reply.started":"2023-12-22T04:50:36.038823Z","shell.execute_reply":"2023-12-22T04:50:37.055307Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# Start training\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:50:37.057901Z","iopub.execute_input":"2023-12-22T04:50:37.058199Z","iopub.status.idle":"2023-12-22T11:19:37.400117Z","shell.execute_reply.started":"2023-12-22T04:50:37.058171Z","shell.execute_reply":"2023-12-22T11:19:37.399172Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231222_045103-yxjen16i</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/shah-imran-1599/huggingface/runs/yxjen16i' target=\"_blank\">woven-eon-5</a></strong> to <a href='https://wandb.ai/shah-imran-1599/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/shah-imran-1599/huggingface' target=\"_blank\">https://wandb.ai/shah-imran-1599/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/shah-imran-1599/huggingface/runs/yxjen16i' target=\"_blank\">https://wandb.ai/shah-imran-1599/huggingface/runs/yxjen16i</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4380' max='4380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4380/4380 6:27:51, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>300</td>\n      <td>1.883700</td>\n      <td>2.056279</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.720500</td>\n      <td>2.037768</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>1.663500</td>\n      <td>2.029567</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>1.624100</td>\n      <td>2.032441</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>1.590600</td>\n      <td>2.037302</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>1.533000</td>\n      <td>2.040788</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>1.515700</td>\n      <td>2.039753</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>1.499800</td>\n      <td>2.041039</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>1.485800</td>\n      <td>2.044991</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>1.462300</td>\n      <td>2.059980</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>1.423900</td>\n      <td>2.066339</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>1.418300</td>\n      <td>2.068497</td>\n    </tr>\n    <tr>\n      <td>3900</td>\n      <td>1.414400</td>\n      <td>2.069053</td>\n    </tr>\n    <tr>\n      <td>4200</td>\n      <td>1.415100</td>\n      <td>2.069249</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=4380, training_loss=1.541053486305829, metrics={'train_runtime': 23339.9904, 'train_samples_per_second': 48.06, 'train_steps_per_second': 0.188, 'total_flos': 7.3239111401472e+16, 'train_loss': 1.541053486305829, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"# Save the fine-tuned model\ntrainer.save_model(\"custom_bengali_gpt2Bengali\")","metadata":{"execution":{"iopub.status.busy":"2023-12-22T11:19:37.401796Z","iopub.execute_input":"2023-12-22T11:19:37.402094Z","iopub.status.idle":"2023-12-22T11:19:38.464528Z","shell.execute_reply.started":"2023-12-22T11:19:37.402069Z","shell.execute_reply":"2023-12-22T11:19:38.463107Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# Load the fine-tuned GPT-2 model for inference\nfinetuned_model = GPT2LMHeadModel.from_pretrained(\"custom_bengali_gpt2Bengali\").to(device)\nfinetuned_model.eval()","metadata":{"execution":{"iopub.status.busy":"2023-12-22T11:23:39.632956Z","iopub.execute_input":"2023-12-22T11:23:39.633368Z","iopub.status.idle":"2023-12-22T11:23:40.379065Z","shell.execute_reply.started":"2023-12-22T11:23:39.633340Z","shell.execute_reply":"2023-12-22T11:23:40.377868Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.0, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"# Input text for completion\ninput_text = \"‡¶õ‡¶æ‡¶Ø‡¶º‡¶æ ‡¶π‡¶Ø‡¶º‡ßá ‡¶Æ‡¶ø‡¶≤‡¶æ‡¶Ø‡¶º‡ßá ‡¶Ø‡¶æ‡¶Ø‡¶º‡ßá \"\n# Tokenize the input text\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n# Generate text completions\nmax_length = 512\noutput_text = finetuned_model.generate(input_ids, max_length=max_length, top_k=50, top_p=.90, do_sample=True, num_return_sequences=2)[0]\n# Decode the generated token IDs to text\ncompleted_text = tokenizer.decode(output_text, skip_special_tokens=True)\n\nprint(\"Input Text:\", input_text)\n# print(\"Completed Text:\", completed_text)\nfor line in completed_text.splitlines():\n    print(line)","metadata":{"execution":{"iopub.status.busy":"2023-12-22T11:56:43.838469Z","iopub.execute_input":"2023-12-22T11:56:43.838882Z","iopub.status.idle":"2023-12-22T11:56:51.894198Z","shell.execute_reply.started":"2023-12-22T11:56:43.838844Z","shell.execute_reply":"2023-12-22T11:56:51.893007Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Input Text: ‡¶õ‡¶æ‡¶Ø‡¶º‡¶æ ‡¶π‡¶Ø‡¶º‡ßá ‡¶Æ‡¶ø‡¶≤‡¶æ‡¶Ø‡¶º‡ßá ‡¶Ø‡¶æ‡¶Ø‡¶º‡ßá \n‡¶õ‡¶æ‡¶Ø‡¶º‡¶æ ‡¶π‡¶Ø‡¶º‡ßá ‡¶Æ‡¶ø‡¶≤‡¶æ‡¶Ø‡¶º‡ßá ‡¶Ø‡¶æ‡¶Ø‡¶º‡ßá ‡¶Ø‡ßá‡¶•‡¶æ‡¶Ø‡¶º ‡¶Ü‡¶ï‡¶æ‡¶∂ ‡¶Ü‡¶≤‡ßã‡¶ï ‡¶¢‡¶æ‡¶≤‡ßá ‡¶Ø‡ßá‡¶•‡¶æ‡¶Ø‡¶º ‡¶Ü‡¶ï‡¶æ‡¶∂‡ßá‡¶∞ ‡¶™‡¶æ‡¶®‡ßá ‡¶ö‡¶æ‡¶π‡¶ø‡¶Ø‡¶º‡¶æ ‡¶ï‡¶æ‡¶ü‡¶ø‡¶¨‡ßá ‡¶∏‡¶®‡ßç‡¶ß‡ßá ‡¶∏‡ßÅ‡¶ñ ‡¶¶‡ßÅ‡¶ñ ‡¶¶‡ßá‡¶¨‡¶æ‡¶∞ ‡¶ï‡¶•‡¶æ ‡¶ó‡ßÅ‡¶≤‡¶ø‡•§  ‡¶π‡¶æ‡¶∏‡¶ø ‡¶ñ‡ßá‡¶≤‡¶æ ‡¶¶‡ßá‡¶ñ‡¶ø‡¶¨‡¶æ‡¶∞‡ßá ‡¶™‡¶æ‡¶á ‡¶®‡¶æ‡¶á ‡¶∏‡ßÅ‡¶ñ ‡¶ï‡¶ø‡¶∏‡ßá‡¶∞ ‡¶Æ‡¶æ‡¶ù‡¶ñ‡¶æ‡¶®‡ßá ‡¶Æ‡¶ø‡¶õ‡ßá ‡¶π‡¶æ‡¶∏‡¶ø ‡¶ñ‡ßá‡¶≤‡¶æ ‡¶¶‡ßá‡¶ñ‡¶ø‡¶¨‡¶æ‡¶∞‡ßá ‡¶™‡¶æ‡¶á‡•§ ‡¶¨‡¶ø‡¶´‡¶≤ ‡¶∏‡ßÅ‡¶ñ‡ßá‡¶∞ ‡¶∏‡¶æ‡¶ß ‡¶ú‡¶æ‡¶ó‡ßá ‡¶Æ‡¶®‡ßá, ‡¶ï‡ßá ‡¶§‡¶æ‡¶∞‡ßá ‡¶∏‡ßÅ‡¶ñ‡ßÄ ‡¶ï‡¶∞‡ßá? ‡¶¶‡ßÇ‡¶∞‡ßá ‡¶™‡ßç‡¶∞‡¶¨‡¶æ‡¶∏‡ßá‡¶∞ ‡¶¨‡¶æ‡¶§‡¶æ‡¶∏‡ßá ‡¶¨‡¶æ‡¶Å‡¶∂‡¶ø‡¶∞ ‡¶∏‡ßÅ‡¶∞ ‡¶∂‡¶ø‡¶•‡¶ø‡¶≤ ‡¶∏‡ßÅ‡¶∞‡ßá ‡¶´‡¶ø‡¶∞‡¶ø‡¶§‡ßá‡¶õ‡ßá ‡¶´‡¶ø‡¶∞‡¶ø‡¶§‡ßá‡¶õ‡ßá ‡¶õ‡ßÅ‡¶ü‡¶ø‡¶Ø‡¶º‡¶æ‡•§ ‡¶ú‡ßÄ‡¶¨‡¶®‡ßá‡¶∞ ‡¶™‡¶•‡ßá ‡¶∏‡ßá‡¶á ‡¶õ‡¶ø‡¶≤ ‡¶≠‡¶æ‡¶≤ ‡¶ï‡¶ø‡¶õ‡ßÅ‡¶¶‡ßÇ‡¶∞‡ßá ‡¶∞‡¶Ø‡¶º‡ßá‡¶õ‡ßá ‡¶π‡ßá‡¶•‡¶æ‡¶Ø‡¶º ‡¶è‡¶ï‡¶æ‡¶ï‡ßÄ ‡¶´‡ßÅ‡¶≤‡¶¨‡¶®‡ßá, ‡¶§‡¶æ‡¶∞ ‡¶™‡¶∞‡ßá ‡¶∏‡ßá‡¶á‡¶ñ‡¶æ‡¶®‡ßá ‡¶π‡¶æ‡¶∞‡¶æ‡¶®‡ßã ‡¶´‡ßÅ‡¶≤ ‡¶õ‡ßÅ‡¶Å‡¶°‡¶º‡ßá ‡¶´‡ßá‡¶≤‡ßá‡¶õ‡¶ø ‡¶∏‡ßÅ‡¶¶‡ßÇ‡¶∞‡ßá‡•§ ‡¶´‡ßÅ‡¶≤‡¶ó‡ßÅ‡¶≤‡¶ø‡¶∞ ‡¶∞‡ßÇ‡¶™‡ßá‡¶∞ ‡¶∞‡ßÇ‡¶™‡¶æ‡¶≤‡¶ø ‡¶∞‡¶ô‡ßá, ‡¶ó‡ßÅ‡¶≤‡¶ø‡¶∞ ‡¶ó‡ßÅ‡¶≤‡¶ø‡¶∞ ‡¶≤‡¶æ‡¶≤‡ßá ‡¶ó‡ßÅ‡¶≤‡¶ø‡¶∞ ‡¶ó‡¶®‡ßç‡¶ß‡ßá ‡¶∏‡ßá‡¶á ‡¶ó‡ßã‡¶≤‡¶æ‡¶™‡ßá‡¶∞ ‡¶Æ‡¶æ‡¶Ø‡¶º‡¶æ‡¶Ø‡¶º, ‡¶π‡¶æ‡¶§‡ßá ‡¶π‡¶æ‡¶§‡ßá ‡¶§‡ßÅ‡¶≤‡ßá‡¶õ‡¶ø‡¶≤‡¶æ‡¶Æ ‡¶≠‡¶æ‡¶≤ ‡¶∏‡¶æ‡¶Ø‡¶º‡¶æ‡¶∏‡¶ø ‡¶≤‡¶æ‡¶¨‡¶£‡ßç‡¶Ø‡•§ ‡¶§‡ßã‡¶Æ‡¶æ‡¶∞‡ßá ‡¶õ‡ßÅ‡¶Å‡¶á‡¶Ø‡¶º‡ßá‡¶õ‡¶ø‡¶≤ ‡¶Æ‡¶®‡ßá ‡¶ú‡¶æ‡¶ó‡¶æ‡¶∞ ‡¶≤‡¶æ‡¶ú‡ßá‡•§ ‡¶§‡ßã‡¶Æ‡¶æ‡¶∞‡ßá ‡¶¶‡¶ø‡¶Ø‡¶º‡ßá‡¶õ‡¶ø‡¶≤ ‡¶Æ‡¶®‡ßá ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶≤‡¶æ‡¶ú‡ßá, ‡¶∏‡ßÅ‡¶∞ ‡¶®‡¶æ ‡¶Æ‡ßá‡¶ñ‡ßá ‡¶´‡ßÅ‡¶≤‡ßá‡¶∞ ‡¶∞‡¶ô‡ßá ‡¶ó‡¶æ‡¶Å‡¶•‡¶æ‡¶Ø‡¶º‡ßá ‡¶™‡ßÅ‡¶∞‡¶æ‡¶®‡ßã ‡¶ó‡ßã‡¶≤‡¶æ‡¶™ ‡¶∂‡ßç‡¶∞‡¶æ‡¶ô‡ßç‡¶ó\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import pipeline, GPT2LMHeadModel, GPT2Tokenizer\n\n# Load the saved model and tokenizer\nmodel_path = \"/kaggle/working/custom_bengali_gpt2Bengali\"\nmodel = GPT2LMHeadModel.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(\"flax-community/gpt2-bengali\")\n\n# Additional kwargs for text generation\ngeneration_kwargs = {\n    \"max_length\": max_length,\n    \"num_return_sequences\": 1,\n    \"temperature\": 0.8,  # Example additional parameter\n    \"top_k\": 50,\n    \"top_p\": 0.90,\n    \"do_sample\": True,\n    \"num_return_sequences\": 2\n}\n\n# Create a text generation pipeline with additional kwargs\ntext_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, **generation_kwargs)\n\n# Example usage\ninput_text = \"‡¶õ‡¶æ‡¶Ø‡¶º‡¶æ ‡¶π‡¶Ø‡¶º‡ßá ‡¶Æ‡¶ø‡¶≤‡¶æ‡¶Ø‡¶º‡ßá ‡¶Ø‡¶æ‡¶Ø‡¶º‡ßá\"\ngenerated_text = text_generator(input_text)[0]['generated_text']\n\nprint(\"Input Text:\", input_text)\nprint(\"Generated Text:\", generated_text)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-22T12:09:38.813409Z","iopub.execute_input":"2023-12-22T12:09:38.814304Z","iopub.status.idle":"2023-12-22T12:10:10.488556Z","shell.execute_reply.started":"2023-12-22T12:09:38.814258Z","shell.execute_reply":"2023-12-22T12:10:10.487411Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Input Text: ‡¶õ‡¶æ‡¶Ø‡¶º‡¶æ ‡¶π‡¶Ø‡¶º‡ßá ‡¶Æ‡¶ø‡¶≤‡¶æ‡¶Ø‡¶º‡ßá ‡¶Ø‡¶æ‡¶Ø‡¶º‡ßá\nGenerated Text: ‡¶õ‡¶æ‡¶Ø‡¶º‡¶æ ‡¶π‡¶Ø‡¶º‡ßá ‡¶Æ‡¶ø‡¶≤‡¶æ‡¶Ø‡¶º‡ßá ‡¶Ø‡¶æ‡¶Ø‡¶º‡ßá ‡¶¶‡ßá‡¶ñ‡¶ø‡¶§‡ßá ‡¶™‡¶æ‡¶á ‡¶Æ‡¶æ‡¶ù‡ßá ‡¶Æ‡¶æ‡¶ù‡ßá ‡¶¶‡ßÅ‡¶á ‡¶§‡ßÄ‡¶∞‡ßá ‡¶¶‡ßÅ‡¶á ‡¶∂‡¶ø‡¶∂‡¶ø‡¶∞-‡¶®‡ßÄ‡¶∞‡ßá ‡¶¨‡¶æ‡¶Ø‡¶º‡ßÅ‡¶≠‡¶∞‡ßá ‡¶Æ‡ßá‡¶ò‡ßá‡¶∞ ‡¶Æ‡¶æ‡¶Ø‡¶º‡¶æ‡¶Æ‡¶Ø‡¶º ‡¶ï‡¶æ‡¶®‡¶®-‡¶™‡¶•‡ßá ‡¶¨‡¶æ‡¶Ø‡¶º‡ßÅ‡¶≠‡¶∞‡ßá ‡¶∏‡ßç‡¶®‡¶ø‡¶ó‡ßç‡¶ß ‡¶∏‡¶æ‡¶Ø‡¶º‡¶æ‡¶π‡ßç‡¶®-‡¶ò‡ßá‡¶∞‡¶æ ‡¶¶‡ßÅ‡¶ü‡¶ø ‡¶Æ‡ßá‡¶ò‡ßá‡¶∞ ‡¶¶‡ßÅ‡¶á ‡¶¨‡¶ø‡¶®‡ßç‡¶¶‡ßÅ ‡¶Æ‡¶ø‡¶≤‡ßá ‡¶è‡¶ï‡¶ü‡¶ø ‡¶Æ‡¶æ‡¶Ø‡¶º‡¶æ‡¶Æ‡¶Ø‡¶º ‡¶¨‡¶æ‡¶∏‡¶∞-‡¶ò‡¶∞‡ßá ‡¶®‡¶æ‡¶π‡¶ø ‡¶Ø‡ßá‡¶§‡ßá ‡¶¶‡ßá‡¶∞‡¶ø, ‡¶ï‡ßá‡¶¨‡¶≤‡¶ø ‡¶ó‡¶æ‡¶® ‡¶ó‡ßá‡¶Ø‡¶º‡ßá ‡¶ó‡ßá‡¶≤‡ßá‡¶Æ ‡¶∏‡¶æ‡¶∞‡¶æ‡¶∞‡¶æ‡¶§‡¶ø‡•§  ‡¶Ø‡ßá ‡¶ó‡¶æ‡¶®‡ßá‡¶∞ ‡¶∏‡ßÅ‡¶∞‡ßá ‡¶§‡ßã‡¶Æ‡¶æ‡¶∞ ‡¶Æ‡¶® ‡¶¨‡ßá‡¶ú‡ßá‡¶õ‡¶ø‡¶≤, ‡¶§‡ßã‡¶Æ‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶æ‡¶£ ‡¶®‡¶æ‡¶π‡¶ø ‡¶ú‡¶æ‡¶®‡¶ø, ‡¶∏‡ßá‡¶á ‡¶ó‡¶æ‡¶®‡ßá‡¶∞ ‡¶∏‡ßÅ‡¶∞‡ßá ‡¶¨‡ßá‡¶¶‡¶®‡¶æ ‡¶ó‡ßá‡¶Å‡¶•‡ßá ‡¶∏‡ßç‡¶Æ‡ßÉ‡¶§‡¶ø‡¶∞ ‡¶¨‡¶®‡ßç‡¶ß‡¶® ‡¶§‡ßã‡¶Æ‡¶æ‡¶∞‡¶ø ‡¶ö‡¶∞‡¶£‡ßá ‡¶õ‡¶ø‡¶®‡ßç‡¶® ‡¶ï‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ‡¶õ‡¶ø ‡¶™‡ßç‡¶∞‡¶æ‡¶£‡ßá‡¶∞ ‡¶Æ‡¶®‡•§ ‡¶ï‡¶æ‡¶®‡¶®‡ßá‡¶∞ ‡¶∏‡ßÅ‡¶ß‡¶æ‡¶∏‡ßç‡¶∞‡ßã‡¶§‡ßá ‡¶¨‡¶æ‡¶Ø‡¶º‡ßÅ‡¶∞ ‡¶Æ‡ßÉ‡¶¶‡ßÅ ‡¶¨‡ßá‡¶ó‡ßá ‡¶∏‡ßç‡¶Æ‡ßÉ‡¶§‡¶ø‡¶∞ ‡¶ù‡¶Ç‡¶ï‡¶æ‡¶∞‡ßá ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶π‡ßÉ‡¶¶‡¶Ø‡¶º ‡¶§‡ßã‡¶Æ‡¶æ‡¶∞‡ßá ‡¶¨‡¶æ‡¶ú‡¶æ‡¶≤‡ßã ‡¶§‡ßÄ‡¶∞‡ßá ‡¶¶‡ßÅ‡¶á ‡¶∏‡ßç‡¶∞‡ßã‡¶§‡ßá‡¶∞ ‡¶Æ‡¶æ‡¶ù‡ßá ‡¶Æ‡¶ø‡¶≤‡¶ø‡¶§‡ßá ‡¶π‡¶¨‡ßá ‡¶¨‡¶ø‡¶∂‡ßç‡¶¨-‡¶π‡ßÉ‡¶¶‡¶Ø‡¶º‡•§ ‡¶™‡ßÅ‡¶≤‡¶ï-‡¶∞‡ßá‡¶£‡ßÅ‡¶∞‡ßá # ‡¶ó‡¶æ‡¶Å‡¶•‡¶ø‡¶≤‡ßá ‡¶§‡ßã‡¶Æ‡¶æ‡¶∞ ‡¶∏‡ßç‡¶¨‡¶∞ ‡¶§‡ßã‡¶Æ‡¶æ‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶Æ‡¶ø‡¶∂‡ßá ‡¶®‡¶ø‡¶∞‡¶®‡ßç‡¶§‡¶∞ ‡¶ö‡¶≤‡¶æ‡¶∞ ‡¶™‡¶•‡ßá ‡¶™‡¶æ‡¶∞‡¶æ‡¶¨‡¶æ‡¶∞‡ßá ‡¶ï‡¶ø‡¶¨‡¶æ‡¶∞‡ßá ‡¶¨‡¶æ‡¶∞‡ßá ‡¶§‡ßã‡¶Æ‡¶æ‡¶∞ ‡¶ö‡¶™‡¶≤ ‡¶π‡¶æ‡¶§‡ßá ‡¶§‡¶æ‡¶á ‡¶¨‡¶æ‡¶∞‡ßá ‡¶¨‡¶æ‡¶∞‡ßá ‡¶Æ‡ßÅ‡¶ï‡ßÅ‡¶≤‡ßá ‡¶ï‡ßÅ‡¶π‡ßÅ‡¶∞‡ßá ‡¶Æ‡ßÅ‡¶ï\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install gradio","metadata":{"execution":{"iopub.status.busy":"2023-12-22T12:16:10.396086Z","iopub.execute_input":"2023-12-22T12:16:10.396508Z","iopub.status.idle":"2023-12-22T12:16:49.663966Z","shell.execute_reply.started":"2023-12-22T12:16:10.396476Z","shell.execute_reply":"2023-12-22T12:16:49.662705Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"Collecting gradio\n  Obtaining dependency information for gradio from https://files.pythonhosted.org/packages/0d/b8/a21fcefdd25b0e7a0fea866d6bbef09c36764f277c4d65238e6b66dd6532/gradio-4.11.0-py3-none-any.whl.metadata\n  Downloading gradio-4.11.0-py3-none-any.whl.metadata (17 kB)\nCollecting aiofiles<24.0,>=22.0 (from gradio)\n  Obtaining dependency information for aiofiles<24.0,>=22.0 from https://files.pythonhosted.org/packages/c5/19/5af6804c4cc0fed83f47bff6e413a98a36618e7d40185cd36e69737f3b0e/aiofiles-23.2.1-py3-none-any.whl.metadata\n  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\nRequirement already satisfied: altair<6.0,>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (5.2.0)\nRequirement already satisfied: fastapi in /opt/conda/lib/python3.10/site-packages (from gradio) (0.101.1)\nCollecting ffmpy (from gradio)\n  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting gradio-client==0.7.3 (from gradio)\n  Obtaining dependency information for gradio-client==0.7.3 from https://files.pythonhosted.org/packages/78/52/a96eada27a2f711464c4a8c85a6110d46e35034cd2108640980c1fa4e8bb/gradio_client-0.7.3-py3-none-any.whl.metadata\n  Downloading gradio_client-0.7.3-py3-none-any.whl.metadata (7.0 kB)\nCollecting httpx (from gradio)\n  Obtaining dependency information for httpx from https://files.pythonhosted.org/packages/39/9b/4937d841aee9c2c8102d9a4eeb800c7dad25386caabb4a1bf5010df81a57/httpx-0.26.0-py3-none-any.whl.metadata\n  Downloading httpx-0.26.0-py3-none-any.whl.metadata (7.6 kB)\nRequirement already satisfied: huggingface-hub>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.19.4)\nRequirement already satisfied: importlib-resources<7.0,>=1.3 in /opt/conda/lib/python3.10/site-packages (from gradio) (5.13.0)\nRequirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.1.2)\nRequirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.1.3)\nRequirement already satisfied: matplotlib~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.7.4)\nRequirement already satisfied: numpy~=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (1.24.3)\nRequirement already satisfied: orjson~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.9.5)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from gradio) (21.3)\nRequirement already satisfied: pandas<3.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (1.5.3)\nRequirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (10.1.0)\nCollecting pydantic>=2.0 (from gradio)\n  Obtaining dependency information for pydantic>=2.0 from https://files.pythonhosted.org/packages/dd/b7/9aea7ee6c01fe3f3c03b8ca3c7797c866df5fecece9d6cb27caa138db2e2/pydantic-2.5.3-py3-none-any.whl.metadata\n  Downloading pydantic-2.5.3-py3-none-any.whl.metadata (65 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: pydub in /opt/conda/lib/python3.10/site-packages (from gradio) (0.25.1)\nCollecting python-multipart (from gradio)\n  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pyyaml<7.0,>=5.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (6.0.1)\nCollecting semantic-version~=2.0 (from gradio)\n  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\nCollecting tomlkit==0.12.0 (from gradio)\n  Obtaining dependency information for tomlkit==0.12.0 from https://files.pythonhosted.org/packages/68/4f/12207897848a653d03ebbf6775a29d949408ded5f99b2d87198bc5c93508/tomlkit-0.12.0-py3-none-any.whl.metadata\n  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: typer[all]<1.0,>=0.9 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.9.0)\nRequirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (4.5.0)\nRequirement already satisfied: uvicorn>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.23.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from gradio-client==0.7.3->gradio) (2023.12.2)\nCollecting websockets<12.0,>=10.0 (from gradio-client==0.7.3->gradio)\n  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio) (4.19.0)\nRequirement already satisfied: toolz in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (3.12.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (4.66.1)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.1.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (4.42.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.4.4)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2023.3)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0->gradio) (0.6.0)\nCollecting pydantic-core==2.14.6 (from pydantic>=2.0->gradio)\n  Obtaining dependency information for pydantic-core==2.14.6 from https://files.pythonhosted.org/packages/90/28/3c6843e6b203999be2660d3f114be196f2182dcac533dc764ad320c9184d/pydantic_core-2.14.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading pydantic_core-2.14.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\nCollecting typing-extensions~=4.0 (from gradio)\n  Obtaining dependency information for typing-extensions~=4.0 from https://files.pythonhosted.org/packages/b7/f4/6a90020cd2d93349b442bfcb657d0dc91eee65491600b2cb1d388bc98e6b/typing_extensions-4.9.0-py3-none-any.whl.metadata\n  Downloading typing_extensions-4.9.0-py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio) (8.1.7)\nRequirement already satisfied: colorama<0.5.0,>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio) (0.4.6)\nRequirement already satisfied: shellingham<2.0.0,>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio) (1.5.4)\nRequirement already satisfied: rich<14.0.0,>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio) (13.5.2)\nRequirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn>=0.14.0->gradio) (0.14.0)\nRequirement already satisfied: starlette<0.28.0,>=0.27.0 in /opt/conda/lib/python3.10/site-packages (from fastapi->gradio) (0.27.0)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx->gradio) (3.7.1)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx->gradio) (2023.11.17)\nCollecting httpcore==1.* (from httpx->gradio)\n  Obtaining dependency information for httpcore==1.* from https://files.pythonhosted.org/packages/56/ba/78b0a99c4da0ff8b0f59defa2f13ca4668189b134bd9840b6202a93d9a0f/httpcore-1.0.2-py3-none-any.whl.metadata\n  Downloading httpcore-1.0.2-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx->gradio) (3.4)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx->gradio) (1.3.0)\nRequirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.1.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.7.1)\nRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.30.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.9.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.16.1)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio->httpx->gradio) (1.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.2.0)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->gradio) (1.26.15)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.0)\nDownloading gradio-4.11.0-py3-none-any.whl (16.6 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading gradio_client-0.7.3-py3-none-any.whl (304 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m304.8/304.8 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\nDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\nDownloading pydantic-2.5.3-py3-none-any.whl (381 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m381.9/381.9 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pydantic_core-2.14.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\nDownloading httpx-0.26.0-py3-none-any.whl (75 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: ffmpy\n  Building wheel for ffmpy (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=5bad47d9044198c6d9bf44d555cfdece49d91d6d78d088ca7fc56f232774db91\n  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\nSuccessfully built ffmpy\nInstalling collected packages: ffmpy, websockets, typing-extensions, tomlkit, semantic-version, python-multipart, httpcore, aiofiles, pydantic-core, httpx, pydantic, gradio-client, gradio\n  Attempting uninstall: websockets\n    Found existing installation: websockets 12.0\n    Uninstalling websockets-12.0:\n      Successfully uninstalled websockets-12.0\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.5.0\n    Uninstalling typing_extensions-4.5.0:\n      Successfully uninstalled typing_extensions-4.5.0\n  Attempting uninstall: tomlkit\n    Found existing installation: tomlkit 0.12.3\n    Uninstalling tomlkit-0.12.3:\n      Successfully uninstalled tomlkit-0.12.3\n  Attempting uninstall: pydantic-core\n    Found existing installation: pydantic_core 2.14.5\n    Uninstalling pydantic_core-2.14.5:\n      Successfully uninstalled pydantic_core-2.14.5\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 1.10.12\n    Uninstalling pydantic-1.10.12:\n      Successfully uninstalled pydantic-1.10.12\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ncuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ndask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\njupyterlab 4.0.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.1 requires jupyterlab<5.0.0a0,>=4.0.6, but you have jupyterlab 4.0.5 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.24.3 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.4 which is incompatible.\ntensorflow 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.9.0 which is incompatible.\ntensorflow-probability 0.21.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\ntensorflowjs 4.14.0 requires packaging~=23.1, but you have packaging 21.3 which is incompatible.\nydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.3 which is incompatible.\nydata-profiling 4.5.1 requires pydantic<2,>=1.8.1, but you have pydantic 2.5.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed aiofiles-23.2.1 ffmpy-0.3.1 gradio-4.11.0 gradio-client-0.7.3 httpcore-1.0.2 httpx-0.26.0 pydantic-2.5.2 pydantic-core-2.14.6 python-multipart-0.0.6 semantic-version-2.10.0 tomlkit-0.12.0 typing-extensions-4.7.1 websockets-11.0.3\n","output_type":"stream"}]},{"cell_type":"code","source":"generation_kwargs = {\n    \"max_length\": max_length,\n    \"num_return_sequences\": 1,\n    \"temperature\": 0.8,  # Example additional parameter\n    \"top_k\": 50,\n    \"top_p\": 0.90,\n    \"do_sample\": True,\n    \"num_return_sequences\": 2\n}","metadata":{"execution":{"iopub.status.busy":"2023-12-22T12:19:38.132506Z","iopub.execute_input":"2023-12-22T12:19:38.132987Z","iopub.status.idle":"2023-12-22T12:19:38.140444Z","shell.execute_reply.started":"2023-12-22T12:19:38.132951Z","shell.execute_reply":"2023-12-22T12:19:38.139428Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"!pip install pydantic","metadata":{"execution":{"iopub.status.busy":"2023-12-22T12:22:11.801330Z","iopub.execute_input":"2023-12-22T12:22:11.802377Z","iopub.status.idle":"2023-12-22T12:22:25.636870Z","shell.execute_reply.started":"2023-12-22T12:22:11.802337Z","shell.execute_reply":"2023-12-22T12:22:25.635562Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (2.5.2)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic) (0.6.0)\nCollecting pydantic-core==2.14.5 (from pydantic)\n  Obtaining dependency information for pydantic-core==2.14.5 from https://files.pythonhosted.org/packages/7c/f5/3e59681bd53955da311a7f4efbb6315d01006e9d18b8a06b527a22d3d923/pydantic_core-2.14.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading pydantic_core-2.14.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic) (4.7.1)\nDownloading pydantic_core-2.14.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: pydantic-core\n  Attempting uninstall: pydantic-core\n    Found existing installation: pydantic_core 2.14.6\n    Uninstalling pydantic_core-2.14.6:\n      Successfully uninstalled pydantic_core-2.14.6\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.3 which is incompatible.\nydata-profiling 4.5.1 requires pydantic<2,>=1.8.1, but you have pydantic 2.5.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed pydantic-core-2.14.5\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import pipeline, GPT2LMHeadModel, GPT2Tokenizer\nimport gradio as gr\n\nmodel_path = \"/kaggle/working/custom_bengali_gpt2Bengali\"\nmodel = GPT2LMHeadModel.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(\"flax-community/gpt2-bengali\")\n\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, **generation_kwargs)\n\ndemo = gr.Interface.from_pipeline(pipe)\ndemo.launch()","metadata":{"execution":{"iopub.status.busy":"2023-12-22T12:22:30.960227Z","iopub.execute_input":"2023-12-22T12:22:30.960648Z","iopub.status.idle":"2023-12-22T12:22:31.344789Z","shell.execute_reply.started":"2023-12-22T12:22:30.960615Z","shell.execute_reply":"2023-12-22T12:22:31.343077Z"},"trusted":true},"execution_count":56,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[56], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline, GPT2LMHeadModel, GPT2Tokenizer\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgr\u001b[39;00m\n\u001b[1;32m      4\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/working/custom_bengali_gpt2Bengali\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m GPT2LMHeadModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_path)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/gradio/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_simple_templates\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcomponents\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_utils\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/gradio/_simple_templates/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msimpledropdown\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleDropdown\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msimpletextbox\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleTextbox\n\u001b[1;32m      4\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSimpleDropdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSimpleTextbox\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/gradio/_simple_templates/simpledropdown.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Callable\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FormComponent\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Events\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSimpleDropdown\u001b[39;00m(FormComponent):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/gradio/components/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mannotated_image\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AnnotatedImage\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Audio\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbar_plot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BarPlot\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/gradio/components/annotated_image.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio_client\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocumentation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m document, set_documentation_group\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image \u001b[38;5;28;01mas\u001b[39;00m _Image  \u001b[38;5;66;03m# using _ to minimize namespace pollution\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m processing_utils, utils\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Component\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_classes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FileData, GradioModel\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/gradio/processing_utils.py:22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image, ImageOps, PngImagePlugin\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wasm_utils\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_classes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FileData, GradioModel, GradioRootModel\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m abspath\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings():\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/gradio/data_classes.py:19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wasm_utils\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m wasm_utils\u001b[38;5;241m.\u001b[39mIS_WASM:\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel, RootModel, ValidationError  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# XXX: Currently Pyodide V2 is not available on Pyodide,\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# so we install V1 for the Wasm version.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Generic, TypeVar\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'RootModel' from 'pydantic' (/opt/conda/lib/python3.10/site-packages/pydantic/__init__.cpython-310-x86_64-linux-gnu.so)"],"ename":"ImportError","evalue":"cannot import name 'RootModel' from 'pydantic' (/opt/conda/lib/python3.10/site-packages/pydantic/__init__.cpython-310-x86_64-linux-gnu.so)","output_type":"error"}]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2023-12-22T13:13:59.276569Z","iopub.execute_input":"2023-12-22T13:13:59.277162Z","iopub.status.idle":"2023-12-22T13:13:59.297949Z","shell.execute_reply.started":"2023-12-22T13:13:59.277134Z","shell.execute_reply":"2023-12-22T13:13:59.297087Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29aa66ff3f9b472784e718a73bcd11e6"}},"metadata":{}}]},{"cell_type":"code","source":"trainer.push_to_hub(\"Kaizu07/custom_bengali_gpt2Bengali\")","metadata":{"execution":{"iopub.status.busy":"2023-12-22T13:15:29.598337Z","iopub.execute_input":"2023-12-22T13:15:29.599170Z","iopub.status.idle":"2023-12-22T13:15:29.986034Z","shell.execute_reply.started":"2023-12-22T13:15:29.599135Z","shell.execute_reply":"2023-12-22T13:15:29.984657Z"},"trusted":true},"execution_count":3,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mpush_to_hub(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKaizu07/custom_bengali_gpt2Bengali\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"],"ename":"NameError","evalue":"name 'trainer' is not defined","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}