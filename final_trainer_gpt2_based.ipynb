{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7257462,"sourceType":"datasetVersion","datasetId":4205659},{"sourceId":7258214,"sourceType":"datasetVersion","datasetId":4206151}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:40:39.308544Z","iopub.execute_input":"2023-12-22T04:40:39.309233Z","iopub.status.idle":"2023-12-22T04:40:39.313361Z","shell.execute_reply.started":"2023-12-22T04:40:39.309193Z","shell.execute_reply":"2023-12-22T04:40:39.312378Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"!pip install pandas==1.5.3","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:40:39.659920Z","iopub.execute_input":"2023-12-22T04:40:39.660679Z","iopub.status.idle":"2023-12-22T04:40:52.367558Z","shell.execute_reply.started":"2023-12-22T04:40:39.660641Z","shell.execute_reply":"2023-12-22T04:40:52.366382Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pandas==1.5.3 in /opt/conda/lib/python3.10/site-packages (1.5.3)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas==1.5.3) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas==1.5.3) (2023.3)\nRequirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from pandas==1.5.3) (1.24.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas==1.5.3) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"def save_combined_text(dir_path, output_file):\n    combined_text = \"\"\n    for file_name in os.listdir(dir_path):\n        if file_name.endswith(\".txt\"):\n            with open(os.path.join(dir_path, file_name), 'r', encoding='utf-8') as file:\n                file_content = file.read()\n                combined_text += file_content + \"<|endoftext|>\" + \"\\n\"\n    with open(output_file, 'w', encoding='utf-8') as output:\n        output.write(combined_text)\n    print(\"Combined text saved to:\", output_file)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-22T04:40:52.369926Z","iopub.execute_input":"2023-12-22T04:40:52.370221Z","iopub.status.idle":"2023-12-22T04:40:52.376988Z","shell.execute_reply.started":"2023-12-22T04:40:52.370195Z","shell.execute_reply":"2023-12-22T04:40:52.376035Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"dir_path_train = \"/kaggle/input/bl-splitted2/train\"\noutput_file_train = \"/kaggle/working/bengali_literature_train_gpt2.txt\"\nsave_combined_text(dir_path_train, output_file_train)\n\ndir_path_valid = \"/kaggle/input/bl-splitted2/valid\"\noutput_file_valid = \"/kaggle/working/bengali_literature_valid_gpt2.txt\"\nsave_combined_text(dir_path_valid, output_file_valid)","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:40:52.378269Z","iopub.execute_input":"2023-12-22T04:40:52.378882Z","iopub.status.idle":"2023-12-22T04:40:54.197644Z","shell.execute_reply.started":"2023-12-22T04:40:52.378852Z","shell.execute_reply":"2023-12-22T04:40:54.196489Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Combined text saved to: /kaggle/working/bengali_literature_train_gpt2.txt\nCombined text saved to: /kaggle/working/bengali_literature_valid_gpt2.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\ngpt2_bengali_tokenizer = AutoTokenizer.from_pretrained(\"flax-community/gpt2-bengali\")","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:40:54.199857Z","iopub.execute_input":"2023-12-22T04:40:54.200182Z","iopub.status.idle":"2023-12-22T04:40:54.806778Z","shell.execute_reply.started":"2023-12-22T04:40:54.200154Z","shell.execute_reply":"2023-12-22T04:40:54.805912Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"print(len(gpt2_bengali_tokenizer.get_vocab()))\ngpt2_bengali_tokenizer","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:40:54.807951Z","iopub.execute_input":"2023-12-22T04:40:54.808277Z","iopub.status.idle":"2023-12-22T04:40:54.847574Z","shell.execute_reply.started":"2023-12-22T04:40:54.808250Z","shell.execute_reply":"2023-12-22T04:40:54.846519Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"50256\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"GPT2TokenizerFast(name_or_path='flax-community/gpt2-bengali', vocab_size=50256, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n\t0: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}"},"metadata":{}}]},{"cell_type":"code","source":"# new tokenizer\n# from tokenizers import (decoders, models, normalizers, pre_tokenizers, processors, trainers, Tokenizer)\n# from transformers import GPT2Tokenizer, GPT2TokenizerFast, GPT2Model, GPT2LMHeadModel\n# from transformers import TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n\n# new_tokenizer = Tokenizer(models.BPE())\n# new_tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n# trainer = trainers.BpeTrainer(vocab_size=5000, special_tokens=[\"<|endoftext|>\"])\n# train_file = '/kaggle/working/bengali_literature_train_gpt2.txt'\n# new_tokenizer.train([train_file], trainer=trainer)\n# new_tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n# new_tokenizer.decoder = decoders.ByteLevel()\n\n# new_tokenizer = GPT2TokenizerFast(tokenizer_object=new_tokenizer)\n# new_tokenizer.save_pretrained(\"new_tokenizer_gpt2\")\n# new_tokenizer\n\n# # gpt2 tokenizer\n# gpt2_tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n# print(len(gpt2_tokenizer.get_vocab()))\n# gpt2_tokenizer\n\n# # merge the vocabulary for the extended tokenizer\n# vocab_tokens = list(new_tokenizer.get_vocab())\n# decoded_tokens = [new_tokenizer.decoder.decode([token]) for token in vocab_tokens]\n# print(len(vocab_tokens), len(decoded_tokens))\n# gpt2_tokenizer.add_tokens(decoded_tokens)\n# gpt2_tokenizer.save_pretrained(\"extended_tokenizer_gpt2\")\n# print(len(gpt2_tokenizer.get_vocab()))\n# gpt2_tokenizer\n\n# # validate the changes\n# text = \"কফি, গান আর কাজ নিয়ে \"\n# gpt2_tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n# new_tokenizer = GPT2TokenizerFast.from_pretrained(\"new_tokenizer_gpt2\")\n# extended_tokenizer = GPT2TokenizerFast.from_pretrained(\"extended_tokenizer_gpt2\")\n\n# print(len(gpt2_tokenizer.encode(text)))\n# print(gpt2_tokenizer.encode(text))\n# print(len(new_tokenizer.encode(text)))\n# print(new_tokenizer.encode(text))\n# print(len(extended_tokenizer.encode(text)))\n# print(extended_tokenizer.encode(text))","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:40:54.849091Z","iopub.execute_input":"2023-12-22T04:40:54.849440Z","iopub.status.idle":"2023-12-22T04:40:54.859797Z","shell.execute_reply.started":"2023-12-22T04:40:54.849413Z","shell.execute_reply":"2023-12-22T04:40:54.858881Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"import logging\nimport time\nfrom datasets import Dataset, DatasetDict\n\n# Suppress the warning messages\nlogging.getLogger(\"transformers.tokenization_utils_base\").setLevel(logging.ERROR)\n\n# load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"flax-community/gpt2-bengali\")\ntokenizer.pad_token = tokenizer.eos_token\nprint(tokenizer.vocab_size)\nprint(len(tokenizer))","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:43:15.846992Z","iopub.execute_input":"2023-12-22T04:43:15.847416Z","iopub.status.idle":"2023-12-22T04:43:16.373741Z","shell.execute_reply.started":"2023-12-22T04:43:15.847382Z","shell.execute_reply":"2023-12-22T04:43:16.372771Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"50256\n50256\n","output_type":"stream"}]},{"cell_type":"code","source":"# Fine-tuning data\nfrom datasets import Dataset, DatasetDict\n\ntrain_file = \"/kaggle/working/bengali_literature_train_gpt2.txt\"  # Replace with your training data file\nvalid_file = \"/kaggle/working/bengali_literature_valid_gpt2.txt\"  # Replace with your validation data file\n# Read the text data from the .txt file\nwith open(train_file, 'r', encoding='utf-8') as f:\n    train_data = f.readlines()\nwith open(valid_file, 'r', encoding='utf-8') as f:\n    valid_data = f.readlines()\n\nprint(len(train_data), len(valid_data))","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:43:17.952989Z","iopub.execute_input":"2023-12-22T04:43:17.953407Z","iopub.status.idle":"2023-12-22T04:43:18.712883Z","shell.execute_reply.started":"2023-12-22T04:43:17.953373Z","shell.execute_reply":"2023-12-22T04:43:18.711831Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"901371 9296\n","output_type":"stream"}]},{"cell_type":"code","source":"# Create a Dataset object from the text data\ntrain_dataset = Dataset.from_dict({\"text\": train_data})\nvalid_dataset = Dataset.from_dict({\"text\": valid_data})\n\ndef preprocess_function(examples):\n    out = tokenizer(examples[\"text\"])\n    # out = tokenizer([\" \".join(x) for x in examples[\"text\"]])\n    return out","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:43:34.214927Z","iopub.execute_input":"2023-12-22T04:43:34.215312Z","iopub.status.idle":"2023-12-22T04:43:35.615359Z","shell.execute_reply.started":"2023-12-22T04:43:34.215280Z","shell.execute_reply":"2023-12-22T04:43:35.614486Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"train_dataset","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:43:35.616964Z","iopub.execute_input":"2023-12-22T04:43:35.617285Z","iopub.status.idle":"2023-12-22T04:43:35.623772Z","shell.execute_reply.started":"2023-12-22T04:43:35.617257Z","shell.execute_reply":"2023-12-22T04:43:35.622789Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['text'],\n    num_rows: 901371\n})"},"metadata":{}}]},{"cell_type":"code","source":"# Apply tokenization to the dataset in batches using map\ntrain_dataset = train_dataset.map(preprocess_function, batched=True, num_proc=4, remove_columns=train_dataset.column_names)\ntime.sleep(5)\nvalid_dataset = valid_dataset.map(preprocess_function, batched=True, num_proc=4, remove_columns=valid_dataset.column_names)\ntokenized_datasets = DatasetDict({\"train\": train_dataset, \"valid\":valid_dataset})\n","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:43:39.424684Z","iopub.execute_input":"2023-12-22T04:43:39.425575Z","iopub.status.idle":"2023-12-22T04:45:50.839078Z","shell.execute_reply.started":"2023-12-22T04:43:39.425539Z","shell.execute_reply":"2023-12-22T04:45:50.837915Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"     ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#0:   0%|          | 0/226 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b985efdd67e4467ba3987f9ff47e34bb"}},"metadata":{}},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#1:   0%|          | 0/226 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c229a556bf34d228e3407346f7e242c"}},"metadata":{}},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#2:   0%|          | 0/226 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e311e2039d6465087f6418ee06e371e"}},"metadata":{}},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#3:   0%|          | 0/226 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2d5697a0609420ba159046362dd2378"}},"metadata":{}},{"name":"stdout","text":"       ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#0:   0%|          | 0/3 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b30f66af6fd54ed1aee5cde04b5b7342"}},"metadata":{}},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#2:   0%|          | 0/3 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7798e81e396e4bfc80194ddbc4d796e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#1:   0%|          | 0/3 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bd2718b13544801bc8f9d51f312d963"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#3:   0%|          | 0/3 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8921bb28585e4e58939bfed3a6041952"}},"metadata":{}}]},{"cell_type":"code","source":"train_dataset","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:45:50.841531Z","iopub.execute_input":"2023-12-22T04:45:50.842375Z","iopub.status.idle":"2023-12-22T04:45:50.848942Z","shell.execute_reply.started":"2023-12-22T04:45:50.842333Z","shell.execute_reply":"2023-12-22T04:45:50.847890Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'attention_mask'],\n    num_rows: 901371\n})"},"metadata":{}}]},{"cell_type":"code","source":"# Group the tokenized datasets into blocks \nblock_size = 128\n\ndef group_texts(examples):\n    # Concatenate all texts.\n    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n    total_length = len(concatenated_examples[list(examples.keys())[0]])\n    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n    # customize this part to your needs.\n    if total_length >= block_size:\n        total_length = (total_length // block_size) * block_size\n    # Split by chunks of block_size.\n    result = {\n        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n        for k, t in concatenated_examples.items()\n    }\n    result[\"labels\"] = result[\"input_ids\"].copy() # First note that we duplicate the inputs for our labels. This is because the model of the 🤗 Transformers library apply the shifting to the right, so we don't need to do it manually.\n    return result","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:45:50.850195Z","iopub.execute_input":"2023-12-22T04:45:50.850459Z","iopub.status.idle":"2023-12-22T04:45:50.860827Z","shell.execute_reply.started":"2023-12-22T04:45:50.850435Z","shell.execute_reply":"2023-12-22T04:45:50.860105Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Group the tokenized train dataset\nlm_train_dataset = tokenized_datasets['train'].map(group_texts, batched=True, num_proc=4)\ntime.sleep(2)\n# Group the tokenized valid dataset\nlm_valid_dataset = tokenized_datasets['valid'].map(group_texts, batched=True, num_proc=4)\n\nlm_dataset = DatasetDict({\"train\": lm_train_dataset, \"valid\":lm_valid_dataset})\nlm_dataset","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:45:50.863064Z","iopub.execute_input":"2023-12-22T04:45:50.863361Z","iopub.status.idle":"2023-12-22T04:49:14.696923Z","shell.execute_reply.started":"2023-12-22T04:45:50.863336Z","shell.execute_reply":"2023-12-22T04:49:14.695777Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"     ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#0:   0%|          | 0/226 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26e0087bafea49c3a331ea0a629c6b6c"}},"metadata":{}},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#1:   0%|          | 0/226 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5441071f743c4d5380ae9e19f0fb3a81"}},"metadata":{}},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#2:   0%|          | 0/226 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24542f2e3aee4715a34ced17058ce290"}},"metadata":{}},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#3:   0%|          | 0/226 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"296d22edaf3e4bf09f52457733abeb9a"}},"metadata":{}},{"name":"stdout","text":"      ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#0:   0%|          | 0/3 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"986b9b7b4a734ebcac2f533fcb595f54"}},"metadata":{}},{"name":"stdout","text":"  ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#1:   0%|          | 0/3 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"681672f9507a4173bcec369a815ae9d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#2:   0%|          | 0/3 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"876bad5837e440d6813bdba96268eacb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"#3:   0%|          | 0/3 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89d1ad72d14a4e84803dfd917a759cca"}},"metadata":{}},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 373904\n    })\n    valid: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 3488\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"lm_dataset","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:33:33.479666Z","iopub.status.idle":"2023-12-22T04:33:33.480089Z","shell.execute_reply.started":"2023-12-22T04:33:33.479897Z","shell.execute_reply":"2023-12-22T04:33:33.479917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import DataCollatorForLanguageModeling","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:49:52.130483Z","iopub.execute_input":"2023-12-22T04:49:52.130956Z","iopub.status.idle":"2023-12-22T04:50:08.153790Z","shell.execute_reply.started":"2023-12-22T04:49:52.130918Z","shell.execute_reply":"2023-12-22T04:50:08.152895Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"# Data collator for language modeling\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\nout = data_collator([lm_dataset['train'][i] for i in range(5)])\nfor key in out:\n    print(f\"{key} shape: {out[key].shape}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:50:14.081927Z","iopub.execute_input":"2023-12-22T04:50:14.083165Z","iopub.status.idle":"2023-12-22T04:50:14.221865Z","shell.execute_reply.started":"2023-12-22T04:50:14.083121Z","shell.execute_reply":"2023-12-22T04:50:14.220766Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"input_ids shape: torch.Size([5, 128])\nattention_mask shape: torch.Size([5, 128])\nlabels shape: torch.Size([5, 128])\n","output_type":"stream"}]},{"cell_type":"code","source":"lm_dataset","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:50:20.732229Z","iopub.execute_input":"2023-12-22T04:50:20.732980Z","iopub.status.idle":"2023-12-22T04:50:20.739532Z","shell.execute_reply.started":"2023-12-22T04:50:20.732941Z","shell.execute_reply":"2023-12-22T04:50:20.738310Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 373904\n    })\n    valid: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 3488\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"# Option 1: Load the GPT-2 base model and resize its embeddings to match the vocabulary size of our custom tokenizer. \nfrom transformers import GPT2LMHeadModel\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Load the GPT-2 base model\nmodel = GPT2LMHeadModel.from_pretrained(\"flax-community/gpt2-bengali\").to(device)\n\n# Resize the model's embeddings to match the vocabulary size of our tokenizer\n# def find_multiple(n: int, k: int) -> int:\n#     if n % k == 0:\n#         return n\n#     return n + k - (n % k)\n\n# new_embeddings_size = find_multiple(len(tokenizer), 64)\n# model.resize_token_embeddings(new_embeddings_size)","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:50:23.589749Z","iopub.execute_input":"2023-12-22T04:50:23.590665Z","iopub.status.idle":"2023-12-22T04:50:35.936785Z","shell.execute_reply.started":"2023-12-22T04:50:23.590628Z","shell.execute_reply":"2023-12-22T04:50:35.935635Z"},"trusted":true},"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/510M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8547953a9044e348799875e9729d148"}},"metadata":{}}]},{"cell_type":"code","source":"# # Option 2: freeze certain layers of the model (e.g., except embeddings)\nfreeze_layers = False\n\nif freeze_layers:\n    for name, param in model.named_parameters():\n        if 'transformer.wte' in name:\n            param.requires_grad = True\n        else:\n            param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:50:35.938887Z","iopub.execute_input":"2023-12-22T04:50:35.939196Z","iopub.status.idle":"2023-12-22T04:50:35.944513Z","shell.execute_reply.started":"2023-12-22T04:50:35.939171Z","shell.execute_reply":"2023-12-22T04:50:35.943652Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\n\nargs = TrainingArguments(\n    output_dir=\"custom_bengali_gpt2Bengali/\",\n    overwrite_output_dir=True,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    evaluation_strategy=\"steps\",\n    eval_steps=300,\n    logging_steps=300,\n    gradient_accumulation_steps=4,\n    num_train_epochs=3,\n    weight_decay=0.1,\n    warmup_steps=300,\n    lr_scheduler_type=\"cosine\",\n    learning_rate=5e-4,\n    save_steps=300,\n    push_to_hub=False,\n    save_total_limit=2\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:50:35.945690Z","iopub.execute_input":"2023-12-22T04:50:35.947501Z","iopub.status.idle":"2023-12-22T04:50:36.035918Z","shell.execute_reply.started":"2023-12-22T04:50:35.947446Z","shell.execute_reply":"2023-12-22T04:50:36.034703Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# Initialize the Trainer\ntrainer = Trainer(\n    model=model,\n    args=args,\n    data_collator=data_collator,\n    train_dataset=lm_dataset[\"train\"],\n    eval_dataset=lm_dataset[\"valid\"],\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:50:36.038398Z","iopub.execute_input":"2023-12-22T04:50:36.038866Z","iopub.status.idle":"2023-12-22T04:50:37.056287Z","shell.execute_reply.started":"2023-12-22T04:50:36.038823Z","shell.execute_reply":"2023-12-22T04:50:37.055307Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# Start training\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-12-22T04:50:37.057901Z","iopub.execute_input":"2023-12-22T04:50:37.058199Z","iopub.status.idle":"2023-12-22T11:19:37.400117Z","shell.execute_reply.started":"2023-12-22T04:50:37.058171Z","shell.execute_reply":"2023-12-22T11:19:37.399172Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231222_045103-yxjen16i</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/shah-imran-1599/huggingface/runs/yxjen16i' target=\"_blank\">woven-eon-5</a></strong> to <a href='https://wandb.ai/shah-imran-1599/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/shah-imran-1599/huggingface' target=\"_blank\">https://wandb.ai/shah-imran-1599/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/shah-imran-1599/huggingface/runs/yxjen16i' target=\"_blank\">https://wandb.ai/shah-imran-1599/huggingface/runs/yxjen16i</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4380' max='4380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4380/4380 6:27:51, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>300</td>\n      <td>1.883700</td>\n      <td>2.056279</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.720500</td>\n      <td>2.037768</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>1.663500</td>\n      <td>2.029567</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>1.624100</td>\n      <td>2.032441</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>1.590600</td>\n      <td>2.037302</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>1.533000</td>\n      <td>2.040788</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>1.515700</td>\n      <td>2.039753</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>1.499800</td>\n      <td>2.041039</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>1.485800</td>\n      <td>2.044991</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>1.462300</td>\n      <td>2.059980</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>1.423900</td>\n      <td>2.066339</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>1.418300</td>\n      <td>2.068497</td>\n    </tr>\n    <tr>\n      <td>3900</td>\n      <td>1.414400</td>\n      <td>2.069053</td>\n    </tr>\n    <tr>\n      <td>4200</td>\n      <td>1.415100</td>\n      <td>2.069249</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=4380, training_loss=1.541053486305829, metrics={'train_runtime': 23339.9904, 'train_samples_per_second': 48.06, 'train_steps_per_second': 0.188, 'total_flos': 7.3239111401472e+16, 'train_loss': 1.541053486305829, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"# Save the fine-tuned model\ntrainer.save_model(\"custom_bengali_gpt2Bengali\")","metadata":{"execution":{"iopub.status.busy":"2023-12-22T11:19:37.401796Z","iopub.execute_input":"2023-12-22T11:19:37.402094Z","iopub.status.idle":"2023-12-22T11:19:38.464528Z","shell.execute_reply.started":"2023-12-22T11:19:37.402069Z","shell.execute_reply":"2023-12-22T11:19:38.463107Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# Load the fine-tuned GPT-2 model for inference\nfinetuned_model = GPT2LMHeadModel.from_pretrained(\"custom_bengali_gpt2Bengali\").to(device)\nfinetuned_model.eval()","metadata":{"execution":{"iopub.status.busy":"2023-12-22T11:23:39.632956Z","iopub.execute_input":"2023-12-22T11:23:39.633368Z","iopub.status.idle":"2023-12-22T11:23:40.379065Z","shell.execute_reply.started":"2023-12-22T11:23:39.633340Z","shell.execute_reply":"2023-12-22T11:23:40.377868Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.0, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"# Input text for completion\ninput_text = \"ছায়া হয়ে মিলায়ে যায়ে \"\n# Tokenize the input text\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n# Generate text completions\nmax_length = 512\noutput_text = finetuned_model.generate(input_ids, max_length=max_length, top_k=50, top_p=.90, do_sample=True, num_return_sequences=2)[0]\n# Decode the generated token IDs to text\ncompleted_text = tokenizer.decode(output_text, skip_special_tokens=True)\n\nprint(\"Input Text:\", input_text)\n# print(\"Completed Text:\", completed_text)\nfor line in completed_text.splitlines():\n    print(line)","metadata":{"execution":{"iopub.status.busy":"2023-12-22T11:56:43.838469Z","iopub.execute_input":"2023-12-22T11:56:43.838882Z","iopub.status.idle":"2023-12-22T11:56:51.894198Z","shell.execute_reply.started":"2023-12-22T11:56:43.838844Z","shell.execute_reply":"2023-12-22T11:56:51.893007Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Input Text: ছায়া হয়ে মিলায়ে যায়ে \nছায়া হয়ে মিলায়ে যায়ে যেথায় আকাশ আলোক ঢালে যেথায় আকাশের পানে চাহিয়া কাটিবে সন্ধে সুখ দুখ দেবার কথা গুলি।  হাসি খেলা দেখিবারে পাই নাই সুখ কিসের মাঝখানে মিছে হাসি খেলা দেখিবারে পাই। বিফল সুখের সাধ জাগে মনে, কে তারে সুখী করে? দূরে প্রবাসের বাতাসে বাঁশির সুর শিথিল সুরে ফিরিতেছে ফিরিতেছে ছুটিয়া। জীবনের পথে সেই ছিল ভাল কিছুদূরে রয়েছে হেথায় একাকী ফুলবনে, তার পরে সেইখানে হারানো ফুল ছুঁড়ে ফেলেছি সুদূরে। ফুলগুলির রূপের রূপালি রঙে, গুলির গুলির লালে গুলির গন্ধে সেই গোলাপের মায়ায়, হাতে হাতে তুলেছিলাম ভাল সায়াসি লাবণ্য। তোমারে ছুঁইয়েছিল মনে জাগার লাজে। তোমারে দিয়েছিল মনে আমার লাজে, সুর না মেখে ফুলের রঙে গাঁথায়ে পুরানো গোলাপ শ্রাঙ্গ\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import pipeline, GPT2LMHeadModel, GPT2Tokenizer\n\n# Load the saved model and tokenizer\nmodel_path = \"/kaggle/working/custom_bengali_gpt2Bengali\"\nmodel = GPT2LMHeadModel.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(\"flax-community/gpt2-bengali\")\n\n# Additional kwargs for text generation\ngeneration_kwargs = {\n    \"max_length\": max_length,\n    \"num_return_sequences\": 1,\n    \"temperature\": 0.8,  # Example additional parameter\n    \"top_k\": 50,\n    \"top_p\": 0.90,\n    \"do_sample\": True,\n    \"num_return_sequences\": 2\n}\n\n# Create a text generation pipeline with additional kwargs\ntext_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, **generation_kwargs)\n\n# Example usage\ninput_text = \"ছায়া হয়ে মিলায়ে যায়ে\"\ngenerated_text = text_generator(input_text)[0]['generated_text']\n\nprint(\"Input Text:\", input_text)\nprint(\"Generated Text:\", generated_text)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-22T12:09:38.813409Z","iopub.execute_input":"2023-12-22T12:09:38.814304Z","iopub.status.idle":"2023-12-22T12:10:10.488556Z","shell.execute_reply.started":"2023-12-22T12:09:38.814258Z","shell.execute_reply":"2023-12-22T12:10:10.487411Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Input Text: ছায়া হয়ে মিলায়ে যায়ে\nGenerated Text: ছায়া হয়ে মিলায়ে যায়ে দেখিতে পাই মাঝে মাঝে দুই তীরে দুই শিশির-নীরে বায়ুভরে মেঘের মায়াময় কানন-পথে বায়ুভরে স্নিগ্ধ সায়াহ্ন-ঘেরা দুটি মেঘের দুই বিন্দু মিলে একটি মায়াময় বাসর-ঘরে নাহি যেতে দেরি, কেবলি গান গেয়ে গেলেম সারারাতি।  যে গানের সুরে তোমার মন বেজেছিল, তোমার প্রাণ নাহি জানি, সেই গানের সুরে বেদনা গেঁথে স্মৃতির বন্ধন তোমারি চরণে ছিন্ন করিয়াছি প্রাণের মন। কাননের সুধাস্রোতে বায়ুর মৃদু বেগে স্মৃতির ঝংকারে আমার হৃদয় তোমারে বাজালো তীরে দুই স্রোতের মাঝে মিলিতে হবে বিশ্ব-হৃদয়। পুলক-রেণুরে # গাঁথিলে তোমার স্বর তোমার সাথে মিশে নিরন্তর চলার পথে পারাবারে কিবারে বারে তোমার চপল হাতে তাই বারে বারে মুকুলে কুহুরে মুক\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install gradio","metadata":{"execution":{"iopub.status.busy":"2023-12-22T12:16:10.396086Z","iopub.execute_input":"2023-12-22T12:16:10.396508Z","iopub.status.idle":"2023-12-22T12:16:49.663966Z","shell.execute_reply.started":"2023-12-22T12:16:10.396476Z","shell.execute_reply":"2023-12-22T12:16:49.662705Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"Collecting gradio\n  Obtaining dependency information for gradio from https://files.pythonhosted.org/packages/0d/b8/a21fcefdd25b0e7a0fea866d6bbef09c36764f277c4d65238e6b66dd6532/gradio-4.11.0-py3-none-any.whl.metadata\n  Downloading gradio-4.11.0-py3-none-any.whl.metadata (17 kB)\nCollecting aiofiles<24.0,>=22.0 (from gradio)\n  Obtaining dependency information for aiofiles<24.0,>=22.0 from https://files.pythonhosted.org/packages/c5/19/5af6804c4cc0fed83f47bff6e413a98a36618e7d40185cd36e69737f3b0e/aiofiles-23.2.1-py3-none-any.whl.metadata\n  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\nRequirement already satisfied: altair<6.0,>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (5.2.0)\nRequirement already satisfied: fastapi in /opt/conda/lib/python3.10/site-packages (from gradio) (0.101.1)\nCollecting ffmpy (from gradio)\n  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting gradio-client==0.7.3 (from gradio)\n  Obtaining dependency information for gradio-client==0.7.3 from https://files.pythonhosted.org/packages/78/52/a96eada27a2f711464c4a8c85a6110d46e35034cd2108640980c1fa4e8bb/gradio_client-0.7.3-py3-none-any.whl.metadata\n  Downloading gradio_client-0.7.3-py3-none-any.whl.metadata (7.0 kB)\nCollecting httpx (from gradio)\n  Obtaining dependency information for httpx from https://files.pythonhosted.org/packages/39/9b/4937d841aee9c2c8102d9a4eeb800c7dad25386caabb4a1bf5010df81a57/httpx-0.26.0-py3-none-any.whl.metadata\n  Downloading httpx-0.26.0-py3-none-any.whl.metadata (7.6 kB)\nRequirement already satisfied: huggingface-hub>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.19.4)\nRequirement already satisfied: importlib-resources<7.0,>=1.3 in /opt/conda/lib/python3.10/site-packages (from gradio) (5.13.0)\nRequirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.1.2)\nRequirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.1.3)\nRequirement already satisfied: matplotlib~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.7.4)\nRequirement already satisfied: numpy~=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (1.24.3)\nRequirement already satisfied: orjson~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.9.5)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from gradio) (21.3)\nRequirement already satisfied: pandas<3.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (1.5.3)\nRequirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (10.1.0)\nCollecting pydantic>=2.0 (from gradio)\n  Obtaining dependency information for pydantic>=2.0 from https://files.pythonhosted.org/packages/dd/b7/9aea7ee6c01fe3f3c03b8ca3c7797c866df5fecece9d6cb27caa138db2e2/pydantic-2.5.3-py3-none-any.whl.metadata\n  Downloading pydantic-2.5.3-py3-none-any.whl.metadata (65 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: pydub in /opt/conda/lib/python3.10/site-packages (from gradio) (0.25.1)\nCollecting python-multipart (from gradio)\n  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pyyaml<7.0,>=5.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (6.0.1)\nCollecting semantic-version~=2.0 (from gradio)\n  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\nCollecting tomlkit==0.12.0 (from gradio)\n  Obtaining dependency information for tomlkit==0.12.0 from https://files.pythonhosted.org/packages/68/4f/12207897848a653d03ebbf6775a29d949408ded5f99b2d87198bc5c93508/tomlkit-0.12.0-py3-none-any.whl.metadata\n  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: typer[all]<1.0,>=0.9 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.9.0)\nRequirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (4.5.0)\nRequirement already satisfied: uvicorn>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.23.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from gradio-client==0.7.3->gradio) (2023.12.2)\nCollecting websockets<12.0,>=10.0 (from gradio-client==0.7.3->gradio)\n  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio) (4.19.0)\nRequirement already satisfied: toolz in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (3.12.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (4.66.1)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.1.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (4.42.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.4.4)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2023.3)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0->gradio) (0.6.0)\nCollecting pydantic-core==2.14.6 (from pydantic>=2.0->gradio)\n  Obtaining dependency information for pydantic-core==2.14.6 from https://files.pythonhosted.org/packages/90/28/3c6843e6b203999be2660d3f114be196f2182dcac533dc764ad320c9184d/pydantic_core-2.14.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading pydantic_core-2.14.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\nCollecting typing-extensions~=4.0 (from gradio)\n  Obtaining dependency information for typing-extensions~=4.0 from https://files.pythonhosted.org/packages/b7/f4/6a90020cd2d93349b442bfcb657d0dc91eee65491600b2cb1d388bc98e6b/typing_extensions-4.9.0-py3-none-any.whl.metadata\n  Downloading typing_extensions-4.9.0-py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio) (8.1.7)\nRequirement already satisfied: colorama<0.5.0,>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio) (0.4.6)\nRequirement already satisfied: shellingham<2.0.0,>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio) (1.5.4)\nRequirement already satisfied: rich<14.0.0,>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio) (13.5.2)\nRequirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn>=0.14.0->gradio) (0.14.0)\nRequirement already satisfied: starlette<0.28.0,>=0.27.0 in /opt/conda/lib/python3.10/site-packages (from fastapi->gradio) (0.27.0)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx->gradio) (3.7.1)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx->gradio) (2023.11.17)\nCollecting httpcore==1.* (from httpx->gradio)\n  Obtaining dependency information for httpcore==1.* from https://files.pythonhosted.org/packages/56/ba/78b0a99c4da0ff8b0f59defa2f13ca4668189b134bd9840b6202a93d9a0f/httpcore-1.0.2-py3-none-any.whl.metadata\n  Downloading httpcore-1.0.2-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx->gradio) (3.4)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx->gradio) (1.3.0)\nRequirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.1.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.7.1)\nRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.30.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.9.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.16.1)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio->httpx->gradio) (1.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.2.0)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->gradio) (1.26.15)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.0)\nDownloading gradio-4.11.0-py3-none-any.whl (16.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading gradio_client-0.7.3-py3-none-any.whl (304 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.8/304.8 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\nDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\nDownloading pydantic-2.5.3-py3-none-any.whl (381 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.9/381.9 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pydantic_core-2.14.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\nDownloading httpx-0.26.0-py3-none-any.whl (75 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: ffmpy\n  Building wheel for ffmpy (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=5bad47d9044198c6d9bf44d555cfdece49d91d6d78d088ca7fc56f232774db91\n  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\nSuccessfully built ffmpy\nInstalling collected packages: ffmpy, websockets, typing-extensions, tomlkit, semantic-version, python-multipart, httpcore, aiofiles, pydantic-core, httpx, pydantic, gradio-client, gradio\n  Attempting uninstall: websockets\n    Found existing installation: websockets 12.0\n    Uninstalling websockets-12.0:\n      Successfully uninstalled websockets-12.0\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.5.0\n    Uninstalling typing_extensions-4.5.0:\n      Successfully uninstalled typing_extensions-4.5.0\n  Attempting uninstall: tomlkit\n    Found existing installation: tomlkit 0.12.3\n    Uninstalling tomlkit-0.12.3:\n      Successfully uninstalled tomlkit-0.12.3\n  Attempting uninstall: pydantic-core\n    Found existing installation: pydantic_core 2.14.5\n    Uninstalling pydantic_core-2.14.5:\n      Successfully uninstalled pydantic_core-2.14.5\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 1.10.12\n    Uninstalling pydantic-1.10.12:\n      Successfully uninstalled pydantic-1.10.12\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ncuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.0 which is incompatible.\ndask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.0 which is incompatible.\njupyterlab 4.0.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.1 requires jupyterlab<5.0.0a0,>=4.0.6, but you have jupyterlab 4.0.5 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.24.3 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.4 which is incompatible.\ntensorflow 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.9.0 which is incompatible.\ntensorflow-probability 0.21.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\ntensorflowjs 4.14.0 requires packaging~=23.1, but you have packaging 21.3 which is incompatible.\nydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.3 which is incompatible.\nydata-profiling 4.5.1 requires pydantic<2,>=1.8.1, but you have pydantic 2.5.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed aiofiles-23.2.1 ffmpy-0.3.1 gradio-4.11.0 gradio-client-0.7.3 httpcore-1.0.2 httpx-0.26.0 pydantic-2.5.2 pydantic-core-2.14.6 python-multipart-0.0.6 semantic-version-2.10.0 tomlkit-0.12.0 typing-extensions-4.7.1 websockets-11.0.3\n","output_type":"stream"}]},{"cell_type":"code","source":"generation_kwargs = {\n    \"max_length\": max_length,\n    \"num_return_sequences\": 1,\n    \"temperature\": 0.8,  # Example additional parameter\n    \"top_k\": 50,\n    \"top_p\": 0.90,\n    \"do_sample\": True,\n    \"num_return_sequences\": 2\n}","metadata":{"execution":{"iopub.status.busy":"2023-12-22T12:19:38.132506Z","iopub.execute_input":"2023-12-22T12:19:38.132987Z","iopub.status.idle":"2023-12-22T12:19:38.140444Z","shell.execute_reply.started":"2023-12-22T12:19:38.132951Z","shell.execute_reply":"2023-12-22T12:19:38.139428Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"!pip install pydantic","metadata":{"execution":{"iopub.status.busy":"2023-12-22T12:22:11.801330Z","iopub.execute_input":"2023-12-22T12:22:11.802377Z","iopub.status.idle":"2023-12-22T12:22:25.636870Z","shell.execute_reply.started":"2023-12-22T12:22:11.802337Z","shell.execute_reply":"2023-12-22T12:22:25.635562Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (2.5.2)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic) (0.6.0)\nCollecting pydantic-core==2.14.5 (from pydantic)\n  Obtaining dependency information for pydantic-core==2.14.5 from https://files.pythonhosted.org/packages/7c/f5/3e59681bd53955da311a7f4efbb6315d01006e9d18b8a06b527a22d3d923/pydantic_core-2.14.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading pydantic_core-2.14.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\nRequirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic) (4.7.1)\nDownloading pydantic_core-2.14.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: pydantic-core\n  Attempting uninstall: pydantic-core\n    Found existing installation: pydantic_core 2.14.6\n    Uninstalling pydantic_core-2.14.6:\n      Successfully uninstalled pydantic_core-2.14.6\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.3 which is incompatible.\nydata-profiling 4.5.1 requires pydantic<2,>=1.8.1, but you have pydantic 2.5.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed pydantic-core-2.14.5\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import pipeline, GPT2LMHeadModel, GPT2Tokenizer\nimport gradio as gr\n\nmodel_path = \"/kaggle/working/custom_bengali_gpt2Bengali\"\nmodel = GPT2LMHeadModel.from_pretrained(model_path)\ntokenizer = AutoTokenizer.from_pretrained(\"flax-community/gpt2-bengali\")\n\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, **generation_kwargs)\n\ndemo = gr.Interface.from_pipeline(pipe)\ndemo.launch()","metadata":{"execution":{"iopub.status.busy":"2023-12-22T12:22:30.960227Z","iopub.execute_input":"2023-12-22T12:22:30.960648Z","iopub.status.idle":"2023-12-22T12:22:31.344789Z","shell.execute_reply.started":"2023-12-22T12:22:30.960615Z","shell.execute_reply":"2023-12-22T12:22:31.343077Z"},"trusted":true},"execution_count":56,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[56], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline, GPT2LMHeadModel, GPT2Tokenizer\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgr\u001b[39;00m\n\u001b[1;32m      4\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/working/custom_bengali_gpt2Bengali\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m GPT2LMHeadModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_path)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/gradio/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_simple_templates\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcomponents\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_utils\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/gradio/_simple_templates/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msimpledropdown\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleDropdown\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msimpletextbox\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleTextbox\n\u001b[1;32m      4\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSimpleDropdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSimpleTextbox\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/gradio/_simple_templates/simpledropdown.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Callable\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FormComponent\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Events\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSimpleDropdown\u001b[39;00m(FormComponent):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/gradio/components/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mannotated_image\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AnnotatedImage\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Audio\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbar_plot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BarPlot\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/gradio/components/annotated_image.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio_client\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocumentation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m document, set_documentation_group\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image \u001b[38;5;28;01mas\u001b[39;00m _Image  \u001b[38;5;66;03m# using _ to minimize namespace pollution\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m processing_utils, utils\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Component\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_classes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FileData, GradioModel\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/gradio/processing_utils.py:22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image, ImageOps, PngImagePlugin\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wasm_utils\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_classes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FileData, GradioModel, GradioRootModel\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m abspath\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings():\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/gradio/data_classes.py:19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wasm_utils\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m wasm_utils\u001b[38;5;241m.\u001b[39mIS_WASM:\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel, RootModel, ValidationError  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# XXX: Currently Pyodide V2 is not available on Pyodide,\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# so we install V1 for the Wasm version.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Generic, TypeVar\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'RootModel' from 'pydantic' (/opt/conda/lib/python3.10/site-packages/pydantic/__init__.cpython-310-x86_64-linux-gnu.so)"],"ename":"ImportError","evalue":"cannot import name 'RootModel' from 'pydantic' (/opt/conda/lib/python3.10/site-packages/pydantic/__init__.cpython-310-x86_64-linux-gnu.so)","output_type":"error"}]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2023-12-22T13:13:59.276569Z","iopub.execute_input":"2023-12-22T13:13:59.277162Z","iopub.status.idle":"2023-12-22T13:13:59.297949Z","shell.execute_reply.started":"2023-12-22T13:13:59.277134Z","shell.execute_reply":"2023-12-22T13:13:59.297087Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29aa66ff3f9b472784e718a73bcd11e6"}},"metadata":{}}]},{"cell_type":"code","source":"trainer.push_to_hub(\"Kaizu07/custom_bengali_gpt2Bengali\")","metadata":{"execution":{"iopub.status.busy":"2023-12-22T13:15:29.598337Z","iopub.execute_input":"2023-12-22T13:15:29.599170Z","iopub.status.idle":"2023-12-22T13:15:29.986034Z","shell.execute_reply.started":"2023-12-22T13:15:29.599135Z","shell.execute_reply":"2023-12-22T13:15:29.984657Z"},"trusted":true},"execution_count":3,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mpush_to_hub(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKaizu07/custom_bengali_gpt2Bengali\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"],"ename":"NameError","evalue":"name 'trainer' is not defined","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}